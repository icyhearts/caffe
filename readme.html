<!DOCTYPE html>
<!-- 
	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script></h1>
	<h2> <script>cSubSection+=1;cSubSubSection=0; document.write("section ", cSection, ".",cSubSection," "); </script></h2>
	<h3> <script>cSubSubSection+=1; document.write("section ", cSection, ".",cSubSection,".",cSubSubSection," "); </script></h3>
	<h1> <script>cSection+=1;cSubSection=0;cSubSection=0;document.write("section ", cSection, " "); </script></h1>
<table style="width:30%">
<caption> padding, alignment, sizeof </caption>
<tr> <th>type</th> <th>32bit </th>  <th>64 bit</th></tr>
</table>
-->
<html>
	<head>
	<meta charset="UTF-8">
	<link href="./templateNSC.css" rel="stylesheet" type="text/css">
	<script>
var cSection=0;
var cSubSection=0;
var cSubSubSection=0;
var cFigure=0;
	</script>

<script>
	MathJax = {
		tex:{
			inlineMath: [['$', '$'], ['\\(', '\\)']]
		},
		svg:{
			fontCache: 'global'
		}
	};
</script>
<script type="text/javascript" id="MathJax-script" async  src="./MathJax/es5/tex-chtml.js">
</script>


	</head>
	<body>

	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>classes</h1>
<pre>



class PoolingLayer : public Layer&lt;Dtype&gt; {
 public:
  explicit PoolingLayer(const LayerParameter& param)
      : Layer&lt;Dtype&gt;(param) {}
  virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;& bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;& top);
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;& bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;& top);


 protected:
  int kernel_h_, kernel_w_;
  int stride_h_, stride_w_;
  int pad_h_, pad_w_;
  int channels_;
  int height_, width_;
  int pooled_height_, pooled_width_;
  bool global_pooling_;
  Blob&lt;Dtype&gt; rand_idx_;
  Blob&lt;int&gt; max_idx_;
};




<a href="comment.softmax_layer.cpp">softmax layer comment Forward_cpu</a>
template &lt;typename Dtype&gt;
class SoftmaxLayer : public Layer&lt;Dtype&gt; {
 public:
  explicit SoftmaxLayer(const LayerParameter& param)
      : Layer&lt;Dtype&gt;(param) {}
  virtual void Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;& bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;& top);

  virtual inline const char* type() const { return "Softmax"; }
  virtual inline int ExactNumBottomBlobs() const { return 1; }
  virtual inline int ExactNumTopBlobs() const { return 1; }

 protected:
  virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;& bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;& top);
  virtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;& bottom,
      const vector&lt;Blob&lt;Dtype&gt;*&gt;& top);
  virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;& top,
      const vector&lt;bool&gt;& propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;& bottom);
  virtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;& top,
     const vector&lt;bool&gt;& propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;& bottom);

  int outer_num_;//bottom[0]-&gt;count(0, softmax_axis_)
  int inner_num_;//bottom[0]-&gt;count(softmax_axis_ + 1)
  int softmax_axis_;
  /// sum_multiplier is used to carry out sum using BLAS
  Blob&lt;Dtype&gt; sum_multiplier_;//全1向量，长度为bottom[0]-&gt;shape(softmax_axis_)
  /// scale is an intermediate Blob to hold temporary results.
  Blob&lt;Dtype&gt; scale_;// 和输入同形，除了axis这一维度为1
};





template &lt;typename Dtype&gt;
class SoftmaxWithLossLayer : public LossLayer&lt;Dtype&gt; {
  /// The internal SoftmaxLayer used to map predictions to a distribution.
  shared_ptr&lt;Layer&lt;Dtype&gt; &gt; softmax_layer_;
  /// prob stores the output probability predictions from the SoftmaxLayer.
  Blob&lt;Dtype&gt; prob_;
  /// bottom vector holder used in call to the underlying SoftmaxLayer::Forward
  vector&lt;Blob&lt;Dtype&gt;*&gt; softmax_bottom_vec_;
  /// top vector holder used in call to the underlying SoftmaxLayer::Forward
  vector&lt;Blob&lt;Dtype&gt;*&gt; softmax_top_vec_;
  /// Whether to ignore instances with a certain label.
  bool has_ignore_label_;
  /// The label indicating that an instance should be ignored.
  int ignore_label_;
  /// How to normalize the output loss.
  LossParameter_NormalizationMode normalization_;

  int softmax_axis_, outer_num_, inner_num_;
};

template &lt;typename Dtype&gt;
class BaseConvolutionLayer : public Layer&lt;Dtype&gt; {

  /// @brief The spatial dimensions of the convolution input.
  Blob&lt;int&gt; conv_input_shape_;

  int num_spatial_axes_;
  int bottom_dim_;//=
  int top_dim_;
  /// @brief The spatial dimensions of the output.
  vector&lt;int&gt; output_shape_;
  const vector&lt;int&gt;* bottom_shape_;

  int channel_axis_;
  int num_;
  int channels_;
  int group_;
  int out_spatial_dim_;//top[0]-&gt;count(first_spatial_axis)
  int weight_offset_;// weight_offset_ = conv_out_channels_ * kernel_dim_ / group_;= Cout*Cin*Hk*Wk/group_
  int num_output_;// output channel
  bool bias_term_;
  bool is_1x1_;
  bool force_nd_im2col_;

  int num_kernels_im2col_;
  int num_kernels_col2im_;
  int conv_out_channels_;
  int conv_in_channels_;// C_i
</pre>
// $H_{out}\times W_{out}$ :
<pre>
  int conv_out_spatial_dim_;//conv_out_spatial_dim_ = top[0]-&gt;count(first_spatial_axis);
// first_spatial_axis=2; N C |H W
</pre> 
  //$C_{in}\times H_k \times W_k$ :
<pre>
  int kernel_dim_;// kernel_dim_ = this-&gt;blobs_[0]-&gt;count(1)
  int col_offset_;//kernel_dim_ * conv_out_spatial_dim_
  int output_offset_;// output_offset_ = conv_out_channels_ * conv_out_spatial_dim_ / group_= Cout*Hout*Wout

  Blob&lt;Dtype&gt; col_buffer_;
  Blob&lt;Dtype&gt; bias_multiplier_;
  /// @brief The spatial dimensions of the col_buffer.
  vector&lt;int&gt; col_buffer_shape_;


};


template &lt;typename Dtype&gt;
class ConvolutionLayer : public BaseConvolutionLayer&lt;Dtype&gt; {
 public:
};
//

template&lt;typename T&gt;
class BlockingQueue {
 public:
  explicit BlockingQueue();
  void push(const T& t);
  bool try_pop(T* t);
  // This logs a message if the threads needs to be blocked
  // useful for detecting e.g. when data feeding is too slow
  T pop(const string& log_on_wait = "");
  bool try_peek(T* t);
  // Return element without removing it
  T peek();
  size_t size() const;
 protected:
  /**
   Move synchronization fields out instead of including boost/thread.hpp
   to avoid a boost/NVCC issues (#1009, #1010) on OSX. Also fails on
   Linux CUDA 7.0.18.
   */
  class sync;
  std::queue&lt;T&gt; queue_;
  shared_ptr&lt;sync&gt; sync_;
DISABLE_COPY_AND_ASSIGN(BlockingQueue);
};

template &lt;typename Dtype&gt;
class Batch {
 public:
  Blob&lt;Dtype&gt; data_, label_;
};

template &lt;typename Dtype&gt;
class BasePrefetchingDataLayer :
    public BaseDataLayer&lt;Dtype&gt;, public InternalThread {
  virtual void InternalThreadEntry();
  virtual void load_batch(Batch&lt;Dtype&gt;* batch) = 0;

  vector&lt;shared_ptr&lt;Batch&lt;Dtype&gt; &gt; &gt; prefetch_;
  BlockingQueue&lt;Batch&lt;Dtype&gt;*&gt; prefetch_free_;
  BlockingQueue&lt;Batch&lt;Dtype&gt;*&gt; prefetch_full_;
  Batch&lt;Dtype&gt;* prefetch_current_;

};
<img src="documentImage/classcaffe_1_1BasePrefetchingDataLayer__inherit__graph.png">
<a href="documentImage/softmax_cross_entropy_loss_IMG_20200417_111430.jpg">softmax带cross entropy loss的求导 </a>
参考文献：https://blog.csdn.net/chaipp0607/article/details/101946040
template &lt;typename Dtype&gt;
class SigmoidCrossEntropyLossLayer : public LossLayer&lt;Dtype&gt; {
  /// sigmoid_output stores the output of the SigmoidLayer.
  shared_ptr&lt;Blob&lt;Dtype&gt; &gt; sigmoid_output_;
  Dtype normalizer_;
  int outer_num_, inner_num_;
  /// How to normalize the loss.
  LossParameter_NormalizationMode normalization_;
  /// Whether to ignore instances with a certain label.
  bool has_ignore_label_;
  /// The label indicating that an instance should be ignored.
  int ignore_label_;
  /// The internal SigmoidLayer used to map predictions to probabilities.
  shared_ptr&lt;SigmoidLayer&lt;Dtype&gt; &gt; sigmoid_layer_;
  /// bottom vector holder to call the underlying SigmoidLayer::Forward
  vector&lt;Blob&lt;Dtype&gt;*&gt; sigmoid_bottom_vec_;
  /// top vector holder to call the underlying SigmoidLayer::Forward
  vector&lt;Blob&lt;Dtype&gt;*&gt; sigmoid_top_vec_;

};
template &lt;typename Dtype&gt;
class InnerProductLayer : public Layer&lt;Dtype&gt; {
  private:
  // in NxCxHxW
  int M_; // M_=N of input,=batch size
  int K_; // K_ = CxHxW of  input
  int N_; // N_:  num of class in output
  Blob&lt;Dtype&gt; bias_multiplier_;//形状为 batchsize
  bool transpose_;  // if true, assume transposed weights
};


    bias_multiplier_.Reshape(bias_shape);


template &lt;typename Dtype&gt;
class Layer {
  protected:

  /** The vector that indicates whether each top blob has a non-zero weight in
   *  the objective function. */
  vector&lt;Dtype&gt; loss_;
  LayerParameter layer_param_;
  Phase phase_;
  /** Vector indicating whether to compute the diff of each param blob. */
  vector&lt;bool&gt; param_propagate_down_;
  vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; blobs_; //权重
};
从solver中得到Net ptr
class Solver
{
  ActionCallback action_request_function_;
  SolverParameter param_;
  int iter_;
  int current_step_;
  shared_ptr&lt;Net&lt;Dtype&gt; &gt; net_;
  vector&lt;shared_ptr&lt;Net&lt;Dtype&gt; &gt; &gt; test_nets_;
  vector&lt;Callback*&gt; callbacks_;
  Dtype smoothed_loss_;
  Dtype smoothed_loss_;
  vector&lt;Dtype&gt; losses_;
	shared_ptr&lt; Net&lt; Dtype &gt; &gt; net (){return net_; }
  vector&lt;shared_ptr&lt;Net&lt;Dtype&gt; &gt; &gt; test_nets_;
  inline const vector&lt;shared_ptr&lt;Net&lt;Dtype&gt; &gt; &gt;& test_nets() {
    return test_nets_;
  }
  vector&lt;Blob&lt;Dtype&gt;*&gt; learnable_params_;
};


template &lt;typename Dtype&gt;
class SGDSolver : public Solver&lt;Dtype&gt; {
  // history maintains the historical momentum data.
  // update maintains update related data and is not needed in snapshots.
  // temp maintains other information that might be needed in computation
  //   of gradients/updates and is not needed in snapshots
  vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; history_, update_, temp_;
};

从Net中得到Layer ptr
template &lt;typename Dtype&gt;
class Net {
  /// The parameters in the network.
  vector&lt;shared_ptr&lt;Blob&lt;Dtype&gt; &gt; &gt; params_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; learnable_params_;

	vector&lt;vector&lt;bool&gt; &gt; bottom_need_backward_;
	vector&lt;bool&gt; layer_need_backward_; //指示器，与layers_等长，表明需要不需要backward,数据层不需要，后面一般需要
	const shared_ptr&lt; Layer&lt; Dtype &gt; &gt; 	layer_by_name (const string &layer_name) const
	void Net&lt;Dtype&gt;::CopyTrainedLayersFrom(const NetParameter& param){}
	const vector&lt;Blob&lt;Dtype&gt;*&gt;& Forward(Dtype* loss = NULL);
	const shared_ptr&lt;Blob&lt;Dtype&gt; &gt; blob_by_name(const string& blob_name) const;

  /// bottom_vecs stores the vectors containing the input for each layer.
  /// They don't actually host the blobs (blobs_ does), so we simply store
  /// pointers.
  vector&lt;vector&lt;Blob&lt;Dtype&gt;*&gt; &gt; bottom_vecs_;
  /// top_vecs stores the vectors containing the output for each layer
  vector&lt;vector&lt;Blob&lt;Dtype&gt;*&gt; &gt; top_vecs_;


  vector&lt;Callback*&gt; before_forward_;
  vector&lt;Callback*&gt; after_forward_;
  vector&lt;Callback*&gt; before_backward_;
  vector&lt;Callback*&gt; after_backward_;
  Dtype ForwardFromTo(int start, int end);
  vector&lt;shared_ptr&lt;Layer&lt;Dtype&gt; &gt; &gt; layers_;
  map&lt;string, int&gt; layer_names_index_;
  vector&lt;Blob&lt;Dtype&gt;*&gt; net_output_blobs_;
};
</pre>
	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>call graph</h1>
<a href="documentImage/conv2gemm.jpg">卷积转换成gemm</a>
<pre>
caffe::Solver&lt;float&gt;::Solve 
  |caffe::Solver&lt;float&gt;::Step 
    |caffe::Net&lt;float&gt;::ForwardBackward 
      |caffe::Net&lt;float&gt;::Forward 
        |caffe::Net&lt;float&gt;::ForwardFromTo 
          |caffe::Layer&lt;float&gt;::Forward 
          | |caffe::BaseConvolutionLayer&lt;float&gt;::Reshape 
          | | |caffe::ConvolutionLayer&lt;float&gt;::compute_output_shape //改变output_shape_这个vector
          | |
          | |caffe::ConvolutionLayer&lt;float&gt;::Forward_cpu //两个for循环，调用forward_cpu_gemm,
          | | //每调用一次forward_cpu_gemm, bottom_data指针+=bottom_dim_(input的CxHxW),共调用num_=batchSize次
          | | //top_data指针+=top_dim(output的CxHxW)
          |   |caffe::BaseConvolutionLayer&lt;float&gt;::forward_cpu_gemm(const Dtype* input,
          |   |  const Dtype* weights, Dtype* output, bool skip_im2col) //forward_cpu_gemm处理一个batch中一张图的多个通道,
          |   |  //对CxHxW的特征图，通过conv_im2col_cpu转换成col矩阵再用caffe_cpu_gemm把权重矩阵和转换过的col矩阵作矩阵乘法
          |   | |caffe::BaseConvolutionLayer&lt;float&gt;::conv_im2col_cpu// 
          |   |   |caffe::im2col_cpu&lt;float&gt; //caffeLearn/n6_im2col_cpu.cpp练习im2col_cpu. comment.im2col.cpp注释
          |   |caffe::BaseConvolutionLayer&lt;float&gt;::forward_cpu_bias



|caffe::InternalThread::entry 
  |caffe::BasePrefetchingDataLayer&lt;float&gt;::InternalThreadEntry 
    |caffe::DataLayer&lt;float&gt;::load_batch 

/// cpp_xor问题
InnerProductLayer&lt;Dtype&gt;::Forward_cpu
caffe中全连接层计算的是数学上:
input       * weight     = output
Batch x Nin * Nin x Nout = Batch x Nout

其中weight在数学上是Nin x Nout的形式，在内存中是按Nout x Nin的格式存储的
证据：
gdb中blobs_[0]-&gt;shape() = {Nout, Nin}// vector
这也是调用cblas_sgemm的时候，矩阵B的参数有 TransB=CblasTrans的原因

 (全1向量) bias     input         weight(数学上Nin x Nout, 内存中Nout x Nin)
 Batchx1 * 1xNout + Batch x Nin * Nin x Nout
=Batch x Nout + Batch x Nout
=Batch x Nout
main----|ReadSolverParamsFromTextFileOrDie
  |solver::net()
    |Net::layer_by_name("inputdata")
  |MemoryDataLayer&lt;Dtype&gt;::Reset
  |Solver::Solve();
    |Solver&lt;Dtype&gt;::Step
      |caffe::Solver&lt;float&gt;::TestAll
      |caffe::Solver&lt;float&gt;::Test
      |net_-&gt;ForwardBackward()
      | |Net::Forward()
      | | |ForwardFromTo(0, layers_.size() - 1);
      | | | |Layer::Forward(bottom_vecs_[i], top_vecs_[i]);
      | | | | |caffe::BaseDataLayer&lt;float&gt;::Reshape
      | | | | |caffe::MemoryDataLayer&lt;float&gt;::Forward_cpu
      | | | | | |caffe::Blob&lt;float&gt;::Reshape// 改变Blob的shape_data_(shared_ptr&lt;SyncedMemory&gt;) 和 shape_(vector)这两个成员
      | | | | | |top[0]-&gt;Reshape(batch_size_, channels_, height_, width_);
      | | | | | |top[1]-&gt;Reshape(batch_size_, 1, 1, 1);
      | | | | | |top[0]-&gt;set_cpu_data(data_ + pos_ * size_);
      | | | | |   |SyncedMemory::set_cpu_data
      | | | | |     |check_device(); // CPU_ONLY的话不会调用
      | | | | | |top[1]-&gt;set_cpu_data(labels_ + pos_);
      | | | | |InnerProductLayer&lt;Dtype&gt;::Reshape(const vector&lt;Blob&lt;Dtype&gt;*&gt;& bottom,
      | | | |   |Blob::CanonicalAxisIndex
      | | | | |caffe::NeuronLayer&lt;float&gt;::Reshape
      | | | | |caffe::SigmoidCrossEntropyLossLayer&lt;float&gt;::Reshape
      | | | |   |LossLayer&lt;Dtype&gt;::Reshape(bottom,top);//vector<int>loss_shape(0); //Loss layers output a scalar;0 axes.
      | |caffe::Net&lt;float&gt;::Backward
      |   |caffe::Net&lt;float&gt;::BackwardFromTo
      |UpdateSmoothedLoss(loss, start_iter, average_loss)
      |caffe::SGDSolver&lt;float&gt;::ApplyUpdate
        |Normalize(param_id);
        |Regularize(param_id);
        |ComputeUpdateValue(param_id, rate);
        |caffe::Net&lt;float&gt;::Update
          |caffe::Blob&lt;Dtype&gt;::Update()

</pre>



	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>code</h1>
<pre>
namespace caffe {
	class SolverRegistry {
		typedef Solver&lt;Dtype&gt;* (*Creator)(const SolverParameter&);
		typedef std::map&lt;string, Creator&gt; CreatorRegistry;
	}
}

//
src/caffe/solver.cpp
caffe中网络输入输出形状的判断:
./src/caffe/layers/data_layer.cpp:31

</pre>
	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script></h1>
<pre>
caffe.proto提供的类:


message LossParameter {
  // If specified, ignore instances with the given label.
  optional int32 ignore_label = 1;
  // How to normalize the loss for loss layers that aggregate across batches,
  // spatial dimensions, or other dimensions.  Currently only implemented in
  // SoftmaxWithLoss and SigmoidCrossEntropyLoss layers.
  enum NormalizationMode {
    // Divide by the number of examples in the batch times spatial dimensions.
    // Outputs that receive the ignore label will NOT be ignored in computing
    // the normalization factor.
    FULL = 0;
    // Divide by the total number of output locations that do not take the
    // ignore_label.  If ignore_label is not set, this behaves like FULL.
    VALID = 1;
    // Divide by the batch size.
    BATCH_SIZE = 2;
    // Do not normalize the loss.
    NONE = 3;
  }
}

message ReLUParameter {
}
LayerParameter有很多xxxParameter作为数据成员:
message LayerParameter {
  optional string name = 1; // the layer name
  optional string type = 2; // the layer type
  repeated string bottom = 3; // the name of each bottom blob
  repeated string top = 4; // the name of each top blob

  // The train / test phase for computation.
  optional Phase phase = 10;
  optional ReLUParameter relu_param = 123;
  optional SigmoidParameter sigmoid_param = 38;
}

message SolverParameter {
  optional string type = 40 [default = "SGD"];
}

</pre>


	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>Layer class继承关系</h1>
<pre>
<img src="documentImage/classcaffe_1_1NeuronLayer__inherit__graph.png">
</pre>

	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>class Blob</h1>
<pre>
public:
  const Dtype* cpu_data() const{
    return (const Dtype*)data_-&gt;cpu_data();// SyncedMemory::cpu_data
  }
  const Dtype* Blob&lt;Dtype&gt;::cpu_diff() const {
    CHECK(diff_);
    return (const Dtype*)diff_-&gt;cpu_data();// SyncedMemory::cpu_data
  }
 protected:
  shared_ptr&lt;SyncedMemory&gt; data_;
  shared_ptr&lt;SyncedMemory&gt; diff_;
  shared_ptr&lt;SyncedMemory&gt; shape_data_;
  vector&lt;int&gt; shape_;
  int count_;
  int capacity_;
</pre>

	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>class SyncedMemory</h1>
<pre>
class SyncedMemory {
 public:
  enum SyncedHead { UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED };
  SyncedMemory();
  explicit SyncedMemory(size_t size);
  ~SyncedMemory();
  const void* cpu_data();
  void set_cpu_data(void* data);
 private:
  void* cpu_ptr_;
  void* gpu_ptr_;
  size_t size_; // sizeof(DType)*N

  SyncedHead head_;
  bool own_cpu_data_;
  bool cpu_malloc_use_cuda_;
  bool own_gpu_data_;
  int device_;
}
</pre>

	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>Layer的继承关系</h1>
<p>

<img src="documentImage/classcaffe_1_1Layer__inherit__graph.png">
</p>


	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>solver class相关</h1>
<pre>
class SolverRegistry {
 public:
  typedef Solver<Dtype>* (*Creator)(const SolverParameter&);
  typedef std::map<string, Creator> CreatorRegistry;
  static Solver<Dtype>* CreateSolver(const SolverParameter& param);
}

// SolverRegisterer的构造函数调用SolverRegistry::AddCreator
template &lt;typename Dtype&gt;
class SolverRegisterer {
 public:
  SolverRegisterer(const string& type,
      Solver&lt;Dtype&gt;* (*creator)(const SolverParameter&)) {
    // LOG(INFO) &lt;&lt; "Registering solver type: " &lt;&lt; type;
    SolverRegistry&lt;Dtype&gt;::AddCreator(type, creator);
  }
};

    boost::shared_ptr&lt;caffe::Solver&lt;float&gt; &gt; solver(caffe::SolverRegistry&lt;float&gt;::CreateSolver(solver_param));


1,在src/caffe/solvers/adadelta_solver.cpp中调用宏:
INSTANTIATE_CLASS(AdaDeltaSolver); // (1)
REGISTER_SOLVER_CLASS(AdaDelta);//  (2)

(1)其中 INSTANTIATE_CLASS的定义在include/caffe/common.hpp:
#define INSTANTIATE_CLASS(classname) \
  char gInstantiationGuard##classname; \
  template class classname&lt;float&gt;; \
  template class classname&lt;double&gt;

(1)AdaDeltaSolver代入展开后:
char gInstantiationGuardAdaDeltaSolver;
template class AdaDeltaSolver&lt;float&gt;; template class AdaDeltaSolver&lt;double&gt;;


(2) REGISTER_SOLVER_CLASS的定义在include/caffe/solver_factory.hpp中
先定义一个template function名字叫: Creator_##type##Solver (该function new一个type##Solver类对象,返回new出来的指针)
再生成两个static的SolverRegisterer的实例,
实例的构造函数会调用AddCreator(type, creator)
AddCreator把刚才的函数指针加进map中：registry[type] = creator;

具体的定义如下：
#define REGISTER_SOLVER_CREATOR(type, creator)                                 \
  static SolverRegisterer&lt;float&gt; g_creator_f_##type(#type, creator&lt;float&gt;);    \
  static SolverRegisterer&lt;double&gt; g_creator_d_##type(#type, creator&lt;double&gt;)   \

#define REGISTER_SOLVER_CLASS(type)                                            \
  template &lt;typename Dtype&gt;                                                    \
  Solver&lt;Dtype&gt;* Creator_##type##Solver(                                       \
      const SolverParameter& param)                                            \
  {                                                                            \
    return new type##Solver&lt;Dtype&gt;(param);                                     \
  }//定义一个函数                                                                            \
  REGISTER_SOLVER_CREATOR(type, Creator_##type##Solver)//生成两个class的static变量

// main函数中事情的真相:
在main函数中调用CreateSolver(param)的本质是在map中查找元素得到一个函数指针，
(这个函数就是Creator_##type##Solver，通过static变量的构造函数加在map中)
Creator_##type##Solver函数会new一个type##Solver类对象(以param为class的构造函数参数),返回new出来的指针
CreateSolver(const SolverParameter& param){//简化版函数体
    return registry[type](param);
}


</pre>


	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>class MemoryDataLayer</h1>
<pre>
template &lt;typename Dtype&gt;
class MemoryDataLayer : public BaseDataLayer&lt;Dtype&gt; {
  private:
  int batch_size_, channels_, height_, width_, size_;
  Dtype* data_;
  Dtype* labels_;
  int n_;
}


(gdb) p  bottom_vecs_            (gdb) p top_vecs_               
0 input {}                              {0x555555849fc0, 0x555555843040}
1fc6    {0x555555849fc0},               {0x5555558b81c0},               
fc6sig  {0x5555558b81c0},               {0x5555558b81c0},               
fc7     {0x5555558b81c0},               {0x555555861d40},               
loss    {0x555555861d40, 0x555555843040}{0x5555558456f0}                

(gdb) p bottom_need_backward_
{}
{0},
{1},
{1}, 
{1, 0}
//
       bottom_vecs_                     top_vecs_                       
data   {}                               {0x555555882f30, 0x555555883d40}
fc6    {0x555555882f30}                 {0x555555884b60}                
fc6sig {0x555555884b60}                 {0x555555884b60}                
fc7    {0x555555884b60}                 {0x5555558850e0}                
split  {0x5555558850e0}                 {0x5555558aa500, 0x555555884c30}
output {0x5555558aa500}                 {0x5555558aae20}                
loss   {0x555555884c30, 0x555555883d40} {0x5555558abb40}                

</pre>
	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>caffe中cross entropy SigmoidCrossEntropyLossLayer</h1>
<pre>
src/caffe/layers/sigmoid_cross_entropy_loss_layer.cpp中
Forward_cpu计算损失不是采用直接的计算方法,为了数值稳定性，有一定的变通和推导, 推导链接：
caffe 论坛
http://www.caffecn.cn/?/question/25
<a href="documentImage/caffeCrossEntropyLoss.jpg"> 图片</a>
csdn博客
https://blog.csdn.net/u012235274/article/details/51361290
</pre>
	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>model.proto和caffe.proto里面的message对应关系</h1>
<pre>
message LayerParameter:
layer {
  //optional string name = 1; // the layer name
//  optional string type = 2; // the layer type
//  repeated string top = 4; // the name of each top blob
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
//  repeated NetStateRule include = 8;
  include {
//  optional Phase phase = 1;
    phase: TRAIN
  }
//  optional TransformationParameter transform_param = 100;
  transform_param {
//  optional float scale = 1 [default = 1];
    scale: 0.00390625
  }
//  optional DataParameter data_param = 11;
  data_param {
//  optional string source = 1;
    source: "examples/mnist/mnist_train_lmdb"
//  optional uint32 batch_size = 4;
    batch_size: 64
//  optional DB backend = 8 [default = LEVELDB];
    backend: LMDB
  }
}
//
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
//  repeated ParamSpec param = 6;
  param {
//  optional float lr_mult = 3 [default = 1.0];
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
//  optional ConvolutionParameter convolution_param = 106;
  convolution_param {
//  optional uint32 num_output = 1; // The number of outputs for the layer
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
</pre>
	<h1> <script>cSection+=1;cSubSection=0;cSubSubSection=0; document.write("section ", cSection, " "); </script>lenet</h1>
<pre>
class DataLayer : public BasePrefetchingDataLayer<Dtype> {
}
                         bottom_vecs_                    top_vecs_                       bottom_need_backward_     
0 mnist                  {}                              {0x5555558b6c00,0x5555558b6dd0} {}    
1 conv1                  {0x5555558b6c00}                {0x5555558aced0}                {0},  
2 pool1                  {0x5555558aced0}                {0x5555558aeb30}                {1},  
3 conv2                  {0x5555558aeb30}                {0x5555558af3f0}                {1},  
4 pool2                  {0x5555558af3f0}                {0x5555558af250}                {1},  
5 ip1                    {0x5555558af250}                {0x5555558a70f0}                {1},  
6 relu1                  {0x5555558a70f0}                {0x5555558a70f0}                {1},  
7 ip2                    {0x5555558a70f0}                {0x5555558a0e50}                {1},  
8 loss(SoftmaxWithLoss)  {0x5555558a0e50,0x5555558b6dd0} {0x5555558a6340}                {1, 0}

</pre>
	</body>
</html>
